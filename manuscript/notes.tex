\documentclass[twoside]{article}
\usepackage{geometry}
\geometry{paperwidth=148mm,paperheight=210mm,margin=15mm}
\usepackage{graphicx}
\usepackage{stix}
\usepackage{url}
\usepackage{natbib}
%\pagestyle{empty}\thispagestyle{empty}

\title{Compact notes on probability and statistics}
\author{Erik Ø. Sørensen}

\input{commands.tex}

%\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}%[section]
\begin{document}
\maketitle
\sloppy
\frenchspacing



\tableofcontents

\newpage
\noindent This set of compact notes is written to support the August method camp
in probability and statistics for new Ph.D students at NHH Norwegian School of
Economics, and is in no way intended to replace a proper book on the topics or
be subject to independent study.\footnote{When you find typos and inconsistencies,
it would be fantastic if you would register an issue---or a pullrequest---at the
Github repo \url{https://github.com/ErikOSorensen/CompactNotes}.}

This set of notes relies a lot on the two books
I have used for teaching in the last decade: \citet{Hogg2013} and
\citet{Linton2017}. When it comes to the statistics part, econometrics textbooks
will probably be sufficient for almost everyone. For those that need a more
theoretical grounding, a first escalation might be \citet{Rosenthal2006}; the
level of rigour of \citet{Billingsley1995} should be sufficiently high for
everyone.


We will not cover software in the Method Camp, but you can never go wrong by
having a look at \citet{Wickham2017} and their introduction to modern R.


\section{Probability}
Let us define a \textbf{sample space} of all possible outcome of some
event (a set). Example, role of a die: $\calS =\{1,2,3,4,5,6\}$. 
An \textbf{event} is a subset of $\calS$. Events are what we assign probability
to.

The set of possible events must be restricted to a $\mathbf{\sigma}$\textbf{-algebra} $\calA$,
a class of subsets of $\calS$ such that if $A\in\calS$ then $A^c\in\calA$, if $A_1,A_2,\dots\in \calF$,
then $\cup_{i=1}^\infty A_i \in \calA$. 

$P$ is a \textbf{probability set function} if $P(A)\geq0$ for all $A\in\calA$; $P(\emptyset)=0$; $P(\calS)=1$,
and if $A_1,A_2,\dots$ is a series of \textbf{disjoint} sets ($A_i\cap A_j\emptyset$ when $i\neq j$), then
$P(\cup_{i=1}^\infty A_i)=\sum_{i=1}^\infty P(A_i)$.


The triple $(\calS,\calA,P)$ is called a \textbf{probability space}.

For $A$ and $B$ in $\calA$, we define the \textbf{conditional probability} of $A$ given $B$
as \[ P(A|B) = P(A\cap B)/P(B),\] when $P(B)>0$. From the definition, we can find 
\textbf{Bayes rule}
as 
\[ P(B|A) = \frac{ P(A|B)\cdot P(B)}{P(A)}.\]

If $A$ and $B$ are events with positive probability, $A$ and $B$ are \textbf{independent} if
$P(A\cap B) = P(A)\cdot P(B)$, $P(A|B)=P(A)$, and $P(B|A)=P(B)$ (all equivalent). Independence
is symmetric but not transitive.

\section{Random variables}

A \textbf{random variable}  (r.v.) is a function from a sample space to the real numbers,
$X\!\!: \calS \rightarrow \SR$. The probability space on $\calS$ maps to a new
probability space on $\SR$ (there are regularity conditions). 



\subsection{Distribution}
A random variable $X$ is said to have a \textbf{distribution function} (d.f.) $F_X$
such that \[ F_X(s) = P( X\in (-\infty, s]).\] When the derivative of the
distribution function exists, it is called the \textbf{density} of $X$, and
$f_X(s)=F'_X(s)$. We then say $X$ is distributed \textbf{continuously}. General
properties: $f_X(s)\geq0$, $\int_a^b f_X(s)\rd s = P(X\in[a,b])$, and
$\int_{-\infty}^\infty f_X(s) \rd s = 1$. When clear from context, we drop the
subscripts (and sometimes also the limits of integration): $\int\! f(s)\rd s=1$.

For \textbf{discrete} r.v., we define the \textbf{probability mass function} (pmf) as
$p_X(s) = P(X\in\{s\})$ for $s\in\calS$. This is analogous to the density function.

\subsection{Expectation of a random variable}
Let $X$ be a random variable. If $X$ is a continuous r.v.
with pdf $f(x)$ and 
\(\int_{-\infty}^\infty |x|f(x) \rd x < \infty,\)
the \textbf{expectation} of $X$ exists as  
\[\E[X] = \int_{-\infty}^\infty xf(x) \rd x.\]
If $Y$ is a discrete random variable with pmf $p_Y$, 
$\E[Y]=\sum_y yp_Y(y)$.

Having different expressions for discrete and continuous random variables is awkward, since
sometimes we need to do theory for mixed variables or variables where we don't know the 
distribution. To find a more general way to express expectations, we need to extend
our notion of \emph{integration}, this is outside the scope of our method camp. But we sometimes
see expressions such at $\E[X]=\int\! x \rd F_X(x)$
or even $E[X]=\int_\Omega X(\omega) P(\rd \omega)$.

The k-th \textbf{moment} of $X$ is $\E [X^k]$ (when it exists).  We often write $\mu=\E[X]$ (the \textbf{mean}) and
$\sigma^2 = \E[(X-\mu)^2]$ (the \textbf{variance}). The k-th \textbf{central moment} 
is $\E [ (X-\mu)^k]$.

The function expectation $m_X(t)=\E[e^{tX}]$ is called the \textbf{moment generating function}
when it exists in an open ball around zero. When it exists, $m'(0)=\E[X]$, $m''(0)=\E[X^2]$ and
$m^k(0)=\E[X^k]$.

\subsection{Expectation results}
\begin{enumerate}
\item Let $X$ be a random variable and let $m$ be a positive integer. 
Suppos(e that $E[X^m]$ exists. If $k$ is an integer and $k < m$, then
$E[X^k]$ exists.
\item Let $u(X)$ be a nonnegative function of the random variable $X$. If
$E[u(X)]$ exists, then for every positive constant $c$,
$P(u(X)\geq c) \leq \frac{E[u(X)]}{c}$ (\textbf{Markov}).
\item Let the random variable $X$ have a distribution with
finite variance $\sigma^2$. Then for every $k>0$, 
$P(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2}$ (\textbf{Chebyshev}).
\item If $\phi$ is convex on an open interval $I$ and $X$ is a random
variable whose support is contained in $I$ and has finite expectation,
then $ \phi(E[X]) \leq E[\phi(X)]$ (\textbf{Jensen}).




\end{enumerate}

\subsection{Functions of random variables}
If $X$ is a random variable and $g$ is a function, $Y=g(X)$ is another random variable.

In Figure~\ref{fig:frv}a, $Y=g(x)$ is an increasing function. 
Thinking about the distribution of $Y$, if we start from the primitives,
the distribution function for $Y$ is $F_Y(y) = P(Y\leq y)$. We can substitute
the definition of $Y$ into this: $F_Y(y) = P(g(X)\leq y)$, where this
substitution depends on $g$ being increasing. Continuing
the substitutions, $P(g(X) \leq y) = P( g^{-1}(g(X)) \leq g^{-1}(y)) = P(X\leq g^{-1}(y))$,
and we can conclude that $F_Y(y)= F_X(g^{-1}(y))$. The density follows by differentiation.

If instead $Y=h(X)$ and $h$ is decreasing (as in Figure ~\ref{fig:frv}b), we
start with a different inequality: If $h(X) \leq y$, then $X\geq h^{-1}(y)$,
and $F_Y(y) = P(X\geq h^{-1})) = 1 - P(X\leq h^{-1}(y)) = 1 - F_X(h^{-1}(y))$.

Either way, for increasing or decreasing $g$'s, taking derivatives we find that 
$f_Y(y) = f_X(g^{-1}(y)) \cdot | \rd g^{-1}(y)/\rd y|$.
The absolute value of the derivative controls for whether $g$ is increasing or decreasing.


\begin{figure}[tb]
\includegraphics[width=0.95\textwidth]{../graphs/transformation_of_RV}

\caption{Functions of random variables}\label{fig:frv}
\end{figure}



\section{Random vectors}
Given a sample space $\mathcal{C}$. Consider the
two random variables $X_1$ and $X_2$, which assign to each element
$c$ of $\mathcal{C}$ an ordered pair of numbers
$X_1(c)=x_1$, $X_2(c)=x_2$. Then we say that $(X_1,X_2)$ is a
\textbf{random vector}. 

If $(X_1,X_2)$ has density $f_{X_1,X_2}(x_1,x_2)$, the \textbf{marginal density}
of $X_1$ is  $ f_{X_1}(x_1) = \int_{-\infty}^\infty f_{X_1,X_2}(x_1,x_2)\rd x_2$.

If $(X_1,X_2)$ has a density, the \textbf{conditional density} of $X_1$ given $X_2$ is
\[f_{X_1|X_2}(x_1|x_2)=\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)},\]
for $f_{X_2}(x_2)>0$.

The \textbf{conditional expectation} is $\E[X_2|X_1=x_1] = \int_{-\infty}^\infty s
f_{X_2|X_1}(s|x_1) \rd s$.

Let $(X_1,X_2)$ have joint d.f. $F(x_1,x_2)$ and let $X_1$ and $X_2$ 
have marginal d.f.s $F_1(x_1)$
  and $F_2(x_2)$. Then $X_1$ and $X_2$ are \textbf{independent} if and only if 
$F(x_1,x_2)= F_1(x_1) F_2(x_2)$. 

If $X_1$ and $X_2$ are independent and $\E[u(X_1)]$ and $\E[v(X_2)]$ exist, then
$\E[u(X_1)v(X_2)] = \E[u(X_1)]\cdot \E[v(X_2)]$.


Let $(X_1,X_2)$ have joint CDF
  $F(x_1,x_2)$ and let $X_1$ and $X_2$ have marginal CDFs $F_1(x_1)$
  and $F_2(x_2)$. Then $X_1$ and $X_2$ are independent if and only if 
  \[F(x_1,x_2)= F_1(x_1) F_2(x_2),\]
  the joint d.f. being the product of marginal d.f.s. It follows that if the corresponding
  densities exist, $X_1$ and $X_2$ are independent if the joint
  density is the product of marginal densities.



For complete generality, let $Y_1=u_1(X_1,X_2)$ and $Y_2=u_2(X_1,X_2)$ be
one-to-one transformed random variables, and let $w_1$ and $w_2$ be inverses.
Then in general, 
\begin{multline*} \int\!\!\!\int_A f_{X_1,X_2}(x_1,x_2)dx_1 d x_2 \\
= \int\!\!\!\int_B
f_{X_1,X_2} (w_1(y_1,y_2),w_2(y_1,y_2))| \det(\mathbf{J}(y_1,y_2))| d y_1
dy_2,\end{multline*} 
where $\mathbf{J}(y_1,y_2)$ is the Jacobian of $\mathbf{w}(\mathbf{y})$ and $B$ is
the image $B=\mathbf{u}(A)$. This is an application of the general formula
for integration with change of variables in calculus.


A vector $\mathbf{X}=(X_1,\dots,X_n)$ where each element is an
\emph{independent} draw from the same distribution as the random variable $X$ is
known as a \textbf{random sample} of size $n$ of the variable $X$.

\section{Estimation}

\section{Hypothesis testing}
Assume that we have two alternative ways to think of what might
be true.

The distribution of $X$ is $f(x;\theta)$, with parameter
$\theta\in\Omega$, and our ideas about truth can be described by
regions of $\Omega$:
\[ H_0: \theta\in\omega_0 \quad \text{vs $H_1: \theta\in\omega_1$.}
\]

Our task is, based on a sample from the distribution of $X$, determine
whether to keep on insisting on $H_0$ or reject that in favor of
$H_1$. What we want is often to fix the probability of Type I error
(``significance level'') to $\alpha=0.01,0.05,\dots$, and conditional
on that, minimize probability of type II error.

Strictly speaking, we decide on a regions of $(X_1,\dots,X_n)\in C$ in
which to accept or reject $H_0$, but in practice we define
test-statistics $T_n(X_1,\dots,X_n)$ and decide on regions of the test
statistics.
Instead of reporting reject/accept with a given $\alpha$-criterion, papers often report $p$-values (but with
implied fixed $\alpha$). 

\textbf{P-values} are the probability of at least as extreme data given $H_0$,
\[ P( \text{data}| H_0).\]
They do not address the likelihood of $H_0$ being true!
In the classical inference, there are never probabilities attached to hypotheses: Hypotheses are true 
or false, and this is not a sampling issue.
If we want to be Bayesian about hypotheses (have beliefs about them), we need to incorporate the priors: 
\[ P(H_0|\text{data}) = \frac{ P(\text{data}|H_0) P(H_0)}{ P(\text{data}) }.
\]

If we assume that $X$ is normally distributed, we could 
test if $E[X] = \mu_0$ using an expression such as
\[
t_n = \frac{\overline{X}_n - \mu_0}{\sigma/\sqrt{n}}.
\]
If we know $\sigma$, then $t_n$ would be normally distributed $N(0,1)$ under $H_0: E[X]=\mu_0$. 
This would be true even in small samples.
We can modify this most simple t-test in various ways.


When we control the type-I error probability to $\alpha$ and the type-II error probability
to $\beta$, we say that we have \textbf{power} $1-\beta$. 
In order to calculate power, it is necessary to be explicit about the alternative hypothesis 
we consider---or calculate for a range of alternative hypothesis. 
Power is the probability of a significant result under the alternative hypothesis, and is a good thing. 
Among people who design experiments, it is customary to aim for power of 80\% or 90\%.

Assume a simple one-sample, one-sided $t$-test against a null-hypothesis of $E[X]=0$. This will
be significant at $\alpha=5\%$ when $t> 1.64$, with probability
\[
P\left( \frac{\overline{X}_n - \mu_0}{\sigma/\sqrt{n}} > 1.64 | E[X]=\mu_1\right) = 
1 - \Phi\left( 1.64 - \frac{\mu_1}{\sigma/\sqrt{n}}\right).
\]
Only the ratio $\mu_1/\sigma$, often known as the \textbf{effect size}  $\delta$, and the sample
size matters.

In Figure~\ref{fig:power} we see the power of a two-sided, two-sample $t$-test
of $\mu=0$ at different alternative hypotheses about the effect size and at
different total sample size.

\begin{figure}[tb]
\includegraphics[width=\textwidth]{../graphs/powergraph}

\caption{Power for a two-sided, two-sample $t$-test of $H_0: \mu=0$ as the effect 
size and sample size varies ($\alpha=0.05$)}\label{fig:power}
\end{figure}




\section{Asymptotic theory}

\subsection{Convergence in probability}

$\{X_n\}$ is a sequence of random variables, and $X$ is a random variable, both
defined on the same sample space. Now $X_n$ \textbf{converges in probability} to $X$
if, for all $\epsilon>0$,
\[\lim_{n\rightarrow\infty} P( |X_n-X|\geq \epsilon) = 0.\] We say that
$\plim X_n = X$, or that $X_n \xrightarrow{P} X$.

\begin{theorem}
Let $\{X_n\}$ be sequence of iid random variables with common mean $\mu$ and
finite variance $\sigma^2$. If $\overline{X}_n = n^{-1}\sum_{i=1}^n X_i$, then
$\plim \overline{X}_n=\mu$. (Weak law of large numbers)
\end{theorem}

There are a number of results about how probability limits work:
\begin{enumerate}
\item Suppose $\plim X_n=X$ and $\plim Y_n=Y$. Then $\plim X_n+Y_n = X+Y$.
\item Suppose $\plim X_n=X$ and $a$ is a constant. Then $\plim a X_n = aX$.
\item Suppose $\plim X_n=a$ and the function $g$ is continuous at $a$. Then $\plim g(X_n)=g(a)$.
\item Suppose $\plim X_n=X$ and $\plim Y_n=Y$, then $\plim X_nY_n=XY$.
\end{enumerate}

The concept of probability limit is closely tied to the statistical concept
of consistency. Let $X$ be a r.v. with d.f. $F(s,\theta)$, for some $\theta\in\Omega$.
Let $X_1,X_2,\dots,X_n$ be a random sample on $X$, and let $T_n$ be a statistic.
$T_n$ is a \textbf{consistent} estimator of $\theta$ if $\plim T_n = \theta$.

\subsection{Convergence in distribution}
$\{X_n\}$ is a sequence of random variables and $X$ is a random variable,
let $F_{X_n}$ and $F_X$ be the respective distribution functions. Let 
$C(F_X)$ be the set of points at which $F_X$ is continuous. Now $X_n$ \textbf{converges in
distribution} to $X$ if $\lim_{n\rightarrow\infty} F_{X_n}(x) = F_X(x)$ for all
$x\in C(F_X)$, and we sometimes write \[X_n \xrightarrow{D} X.\]

Like for convergence in probability, we have some theorems about convergence in distribution:

\begin{enumerate}
\item If $X_n$ converges to $X$ in probability, then $X_n$ converges to $X$ in distribution.
\item If $X_n$ converges to the constant $b$ in distribution, then $X_n$
converges to $b$ in probability.
\item If $X_n$ converges to $X$ in distribution and $Y_n$ converges in probability to $0$,
then $X_n+Y_n$ converges to $X$ in distribution.
\item If $X_n$ converges to $X$ in distribution and $g$ is a continuous function on the support
of $X$, then $g(X_n)$ converges to $g(X)$ in distribution.
\item $X_n$, $X$, $A_n$, and $B_n$ are random variables and $a$ and $b$ are constants. 
If $X_n$ converges to $X$ in distribution, $\plim(A_n)=a$, and $\plim(B_n)=b$, then
\[ A_n + B_n X_n \xrightarrow{D} a + bX.\]
\end{enumerate}


\subsection{The Central Limit Theorem}
There is a whole literature about generalizing central limit theorems. A basic one:

\begin{theorem}
Let $X_1,X_2,\dots,X_n$ denote a random sample from a distribution with mean
$\mu$ and positive variance $\sigma^2$. Then the random variable \[ Y_n =
\left(\sum_{i=1}^n X_i - n\mu \right)/\sqrt{n}\sigma =
\sqrt{n}(\overline{X}_n-\mu)/\sigma\] converges in distribution to a random
variable with a normal distribution $N(0,1)$. 
\end{theorem}

A simple proof can be constructed for the subset of cases where $X$ has a moment generating
function, the trick is to do a second order Taylor-approximation of the moment generating
function of $X-\mu$. 

The $\mathbf{\Delta}$\textbf{-rule} is often useful in conjunction with the CLT.
It says that 

\begin{theorem}
If $\{X_n\}$ is a sequence of random variables such that $\sqrt{n}(\overline{X}_n-\theta)$
converges to $N(0,\sigma^2)$ in distribution, $g$ is a differentiable function at $\theta$,
and $g'(\theta)\neq 0$, then 
\[ \sqrt{n}\left(g(X_n) - g(\theta)\right) \xrightarrow{D} 
N\left(0, \left(g'(\theta)\right)^2\sigma^2\right).\]
\end{theorem}
This can be proven with a Taylor expansion. 

The $\Delta$-rule is the default approach to calculating the distribution of derived
statistics. If we can show that the CLT applies to some moments or parameters,
and we are interested in a function of these moments or parameters, we can use the 
$\Delta$-rule to calculate the distribution of these functions. Statistical packages
will often do this automatically (example: Stata's \texttt{testnl} command).

\section{Math facts}
In this section, there are some mathematical facts that turn out to be useful
for probability and statistics but isn't really part of statistics itself.

The \textbf{exponential function}, written $e^s$ or $\exp(s)$ is important. Can be defined by the
series expansion \[ e^x = \sum_{k=0}^\infty \frac{x^k}{k!},\] or
\[ e^x = \lim_{n\rightarrow \infty} \left( 1 + \frac{x}{n} \right)^n. \]
It is also the case that  $\rd e^s / \rd s =e^s$.


If $\psi$ is some function such that $\psi(n) \rightarrow 0$ as $n\rightarrow\infty$,
then \[ \lim_{n\rightarrow\infty} \left(1 + \frac{x}{n} + \frac{\psi(n)}{n}  \right)^n = e^x,\]
since the $\psi(n)/n$ term vanishes faster than $x/n$.

A useful formulation of \textbf{Taylor's formula} is that for continuous and at
least twice differentiable functions functions $m\!\!\!:\!\!\SR \rightarrow \SR$, there
exists a number $\xi$ between $0$ and $t$ such that \[ m(t) = m(0) + m'(0) t +
\frac{1}{2} m''(\xi) t^2.\]

\addcontentsline{toc}{section}{References}
\bibliographystyle{standard3}
\bibliography{mmref}

\end{document}